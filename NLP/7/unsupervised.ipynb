{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bb857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(file_path):\n",
    "\n",
    "    file_pointer = codecs.open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    file_contents = file_pointer.readlines()\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for _x in range(len(file_contents)):\n",
    "        line = file_contents.pop(0).strip().split(\" \")\n",
    "\n",
    "        for word in line:\n",
    "            if word != \"\":\n",
    "                tokens.append(word)\n",
    "\n",
    "    file_pointer.close()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(file_path):\n",
    "\n",
    "    file_pointer = codecs.open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    file_contents = file_pointer.readlines()\n",
    "\n",
    "    word_types = []\n",
    "\n",
    "    for _x in range(len(file_contents)):\n",
    "        line = file_contents.pop(0).strip().split(\" \")\n",
    "\n",
    "        for word in line:\n",
    "            if word != \"\" and word not in word_types:\n",
    "                word_types.append(word)\n",
    "\n",
    "    file_pointer.close()\n",
    "\n",
    "    return word_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_words(file_path, n):\n",
    "\n",
    "    chop_off_distance = 25\n",
    "\n",
    "    file_pointer = codecs.open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    file_contents = file_pointer.readlines()\n",
    "\n",
    "    word_list = {}\n",
    "\n",
    "    for _x in range(len(file_contents)):\n",
    "        line = file_contents.pop(0).strip().split(\" \")\n",
    "\n",
    "        for word in line:\n",
    "            if word != \"\":\n",
    "                if word not in word_list:\n",
    "                    word_list[word] = 1\n",
    "                else:\n",
    "                    word_list[word] += 1\n",
    "\n",
    "    word_list = sorted(word_list.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    top_n_words = []\n",
    "\n",
    "    for x in range(n):\n",
    "        top_n_words.append(word_list[x + chop_off_distance][0])\n",
    "\n",
    "    file_pointer.close()\n",
    "\n",
    "    return top_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54294f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors(tokens, unique_words, feature_words):\n",
    "\n",
    "    feature_vectors = []\n",
    "\n",
    "    for each in range(len(unique_words)):\n",
    "        feature_vectors.append([])\n",
    "        for _x in range(2 * len(feature_words)):\n",
    "            feature_vectors[each].append(0)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if i < len(tokens) - 4:\n",
    "            if tokens[i + 1] in feature_words:\n",
    "                feature_vectors[unique_words.index(tokens[i])][feature_words.index(tokens[i + 1])] += 1\n",
    "            if tokens[i + 2] in feature_words:\n",
    "                feature_vectors[unique_words.index(tokens[i])][feature_words.index(tokens[i + 2])] += 1\n",
    "            if tokens[i + 3] in feature_words:\n",
    "                feature_vectors[unique_words.index(tokens[i])][feature_words.index(tokens[i + 3])] += 1\n",
    "            if tokens[i + 4] in feature_words:\n",
    "                feature_vectors[unique_words.index(tokens[i])][feature_words.index(tokens[i + 4])] += 1\n",
    "        if i > 3:\n",
    "            if tokens[i - 1] in feature_words:\n",
    "                feature_vectors[unique_words.index(tokens[i])][\n",
    "                    len(feature_words) + feature_words.index(tokens[i - 1])\n",
    "                ] += 1\n",
    "            if tokens[i - 2] in feature_words:\n",
    "                feature_vectors[unique_words.index(tokens[i])][\n",
    "                    len(feature_words) + feature_words.index(tokens[i - 2])\n",
    "                ] += 1\n",
    "            if tokens[i - 3] in feature_words:\n",
    "                feature_vectors[unique_words.index(tokens[i])][\n",
    "                    len(feature_words) + feature_words.index(tokens[i - 3])\n",
    "                ] += 1\n",
    "            if tokens[i - 4] in feature_words:\n",
    "                feature_vectors[unique_words.index(tokens[i])][\n",
    "                    len(feature_words) + feature_words.index(tokens[i - 4])\n",
    "                ] += 1\n",
    "\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c408ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_clusters_with_data(centroids, feature_vectors):\n",
    "    cluster_data_map = []\n",
    "    for _x in range(len(centroids)):\n",
    "        cluster_data_map.append([])\n",
    "\n",
    "    error = 0\n",
    "\n",
    "    for i, data_point in enumerate(feature_vectors):\n",
    "        max_distance = 999999\n",
    "        closest_cluster = -1\n",
    "        for j, centriod in enumerate(centroids):\n",
    "            if dist(data_point, centriod) < max_distance:\n",
    "                max_distance = dist(data_point, centriod)\n",
    "                closest_cluster = j\n",
    "        cluster_data_map[closest_cluster].append(i)\n",
    "        error += max_distance\n",
    "\n",
    "    return cluster_data_map, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(data_point, centriod):\n",
    "\n",
    "    distance = 0\n",
    "\n",
    "    for x in range(len(data_point)):\n",
    "        distance += (data_point[x] - centriod[x]) ** 2\n",
    "\n",
    "    distance = math.sqrt(distance)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30707b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_centroids(feature_vectors, cluster_data_map):\n",
    "\n",
    "    centroids = []\n",
    "\n",
    "    for each in cluster_data_map:\n",
    "        centroids.append(mean_of_data_points(feature_vectors, each))\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_data_points(feature_vectors, list_of_points):\n",
    "\n",
    "    mean = []\n",
    "    for _x in range(len(feature_vectors[0])):\n",
    "        mean.append(0)\n",
    "\n",
    "    for each in list_of_points:\n",
    "        for i, _x in enumerate(feature_vectors[each]):\n",
    "            mean[i] += feature_vectors[each][i]\n",
    "\n",
    "    for x in range(len(mean)):\n",
    "        if len(list_of_points) != 0:\n",
    "            mean[x] = mean[x] / len(list_of_points)\n",
    "\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a93b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    print(\"Processing... It may takes few second(s)\\n\")\n",
    "\n",
    "    # Path of actual text files.\n",
    "    textfiles = [\"./data/hindi.txt\", \"./data/telugu.txt\", \"./data/kannada.txt\", \"./data/tamil.txt\"]\n",
    "    languages = [\"hindi\", \"telugu\", \"kannada\", \"tamil\"]\n",
    "    file_path = textfiles[int(sys.argv[1])]\n",
    "\n",
    "    # Number of feature words to be used for calculating feature vectors\n",
    "    number_of_top_words = 100\n",
    "    tokens = get_tokens(file_path)\n",
    "    word_types = get_unique_words(file_path)\n",
    "    top_n_words = get_frequent_words(file_path, number_of_top_words)\n",
    "    feature_vectors = get_feature_vectors(tokens, word_types, top_n_words)\n",
    "\n",
    "    clusters = [\n",
    "        \"NN\",\n",
    "        \"NST\",\n",
    "        \"NNP\",\n",
    "        \"PRP\",\n",
    "        \"DEM\",\n",
    "        \"VM\",\n",
    "        \"VAUX\",\n",
    "        \"JJ\",\n",
    "        \"RB\",\n",
    "        \"PSP\",\n",
    "        \"RP\",\n",
    "        \"CC\",\n",
    "        \"WQ\",\n",
    "        \"QF\",\n",
    "        \"QC\",\n",
    "        \"QO\",\n",
    "        \"CL\",\n",
    "        \"INTF\",\n",
    "        \"INJ\",\n",
    "        \"NEG\",\n",
    "        \"UT\",\n",
    "        \"SYM\",\n",
    "        \"COMP\",\n",
    "        \"RDP\",\n",
    "        \"ECH\",\n",
    "        \"UNK\",\n",
    "    ]\n",
    "    number_of_clusters = len(clusters)\n",
    "\n",
    "    # Random word index used to get feature vectors as centroids in K means clustering\n",
    "    selected_word_index = [\n",
    "        1,\n",
    "        94,\n",
    "        109,\n",
    "        44,\n",
    "        77,\n",
    "        131,\n",
    "        156,\n",
    "        406,\n",
    "        244,\n",
    "        444,\n",
    "        14,\n",
    "        29,\n",
    "        295,\n",
    "        806,\n",
    "        157,\n",
    "        362,\n",
    "        855,\n",
    "        781,\n",
    "        100,\n",
    "        494,\n",
    "        2017,\n",
    "        222,\n",
    "        199,\n",
    "        40,\n",
    "        400,\n",
    "        689,\n",
    "    ]\n",
    "    centroids = []\n",
    "    for x in range(number_of_clusters):\n",
    "        centroids.append(feature_vectors[selected_word_index[x]])\n",
    "\n",
    "    # K Means Clustering\n",
    "    for _loop in range(10):\n",
    "        cluster_data_map, total_error = map_clusters_with_data(centroids, feature_vectors)\n",
    "        print(total_error)\n",
    "        centroids = recompute_centroids(feature_vectors, cluster_data_map)\n",
    "\n",
    "    # Output clusters to the output file\n",
    "    file_output = codecs.open(\"./output/\" + languages[int(sys.argv[1])] + \"_clusters.txt\", \"w\", \"utf-8\")\n",
    "    for x in range(len(cluster_data_map)):\n",
    "        file_output.write(\"BEGIN CLUSTER\\n\")\n",
    "        for each in cluster_data_map[x]:\n",
    "            file_output.write(word_types[each] + \"\\n\")\n",
    "        file_output.write(\"END CLUSTER\\n\\n\")\n",
    "    file_output.close()\n",
    "\n",
    "    print(\n",
    "        \"Kindly check ./output/\"\n",
    "        + languages[int(sys.argv[1])]\n",
    "        + \"_clusters.txt for clusters and words in that cluster\\n\"\n",
    "    )\n",
    "\n",
    "    # Create training file based on the clusters created above\n",
    "    file_output = codecs.open(\"./data/\" + languages[int(sys.argv[1])] + \"_training_unsupervised.txt\", \"w\", \"utf-8\")\n",
    "    file_pointer = codecs.open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    file_contents = file_pointer.readlines()\n",
    "\n",
    "    for _ in range(len(file_contents)):\n",
    "        line = file_contents.pop(0).strip().split(\" \")\n",
    "        file_output.write(\"<s> START\\n\")\n",
    "        for word in line:\n",
    "            for i in range(len(cluster_data_map)):\n",
    "                for j in cluster_data_map[i]:\n",
    "                    if word_types[j] == word:\n",
    "                        file_output.write(word + \" C\" + str(i) + \"\\n\")\n",
    "                        break\n",
    "        file_output.write(\"</s> END\\n\")\n",
    "\n",
    "    file_pointer.close()\n",
    "\n",
    "    # Perform HMM based tagging on the test sentences using above training file\n",
    "    helper.main(int(sys.argv[1]), sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53abb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Execution begins here\n",
    "# --------------------------------------------------------------------------\n",
    "#     Description\n",
    "#\n",
    "#    If all the libraries are available in target system, main() is called,\n",
    "#    else program exits gracefully by displaying dependancies.\n",
    "#\n",
    "# --------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import codecs\n",
    "        import math\n",
    "        import os\n",
    "        import random\n",
    "        import sys\n",
    "\n",
    "        import helper\n",
    "\n",
    "        if len(sys.argv) == 3:\n",
    "            main()\n",
    "        else:\n",
    "            print(\"Usage: python unsupervised.py <language> <test_file_path>\")\n",
    "            print(\"Example: python unsupervised.py 0 ./data/hindi_testing.txt\")\n",
    "            print(\"More Info: Check ./Readme - Unsupervised.txt for detailed information\")\n",
    "\n",
    "    except ImportError as error:\n",
    "        print(f\"Couldn't find the module - {error.message[16:]}, kindly install before proceeding.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
