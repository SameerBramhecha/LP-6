{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd1979",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Global Variables definition\n",
    "# --------------------------------------------------------------------------\n",
    "tags = [\n",
    "    \"NN\",\n",
    "    \"NST\",\n",
    "    \"NNP\",\n",
    "    \"PRP\",\n",
    "    \"DEM\",\n",
    "    \"VM\",\n",
    "    \"VAUX\",\n",
    "    \"JJ\",\n",
    "    \"RB\",\n",
    "    \"PSP\",\n",
    "    \"RP\",\n",
    "    \"CC\",\n",
    "    \"WQ\",\n",
    "    \"QF\",\n",
    "    \"QC\",\n",
    "    \"QO\",\n",
    "    \"CL\",\n",
    "    \"INTF\",\n",
    "    \"INJ\",\n",
    "    \"NEG\",\n",
    "    \"UT\",\n",
    "    \"SYM\",\n",
    "    \"COMP\",\n",
    "    \"RDP\",\n",
    "    \"ECH\",\n",
    "    \"UNK\",\n",
    "    \"XC\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb84246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Function: max_connect\n",
    "# --------------------------------------------------------------------------\n",
    "# \tDescription\n",
    "#\n",
    "# \tmax_connect function performs the viterbi decoding. Choosing which tag\n",
    "# \tfor the current word leads to a better tag sequence.\n",
    "#\n",
    "# --------------------------------------------------------------------------\n",
    "def max_connect(x, y, viterbi_matrix, emission, transmission_matrix):\n",
    "    max = -99999\n",
    "    path = -1\n",
    "\n",
    "    for k in range(len(tags)):\n",
    "        val = viterbi_matrix[k][x - 1] * transmission_matrix[k][y]\n",
    "        if val * emission > max:\n",
    "            max = val\n",
    "            path = k\n",
    "    return max, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Function: main\n",
    "# --------------------------------------------------------------------------\n",
    "# \tDescription\n",
    "#\n",
    "# \t1) Unique words are extracted from the training data.\n",
    "# \t2) Count of occurence of each tag is calculated.\n",
    "# \t3) Emission & Transmission matrix are initialized and computed.\n",
    "# \t4) Testing data is read.\n",
    "# \t5) Trellis for viterbi decoding is computed.\n",
    "# \t6) Path is printed on to output file.\n",
    "#\n",
    "# --------------------------------------------------------------------------\n",
    "def main():\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Start of Training Phase\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Path of training files\n",
    "    filepath = [\n",
    "        \"./data/hindi_training.txt\",\n",
    "        \"./data/telugu_training.txt\",\n",
    "        \"./data/kannada_training.txt\",\n",
    "        \"./data/tamil_training.txt\",\n",
    "    ]\n",
    "    languages = [\"hindi\", \"telugu\", \"kannada\", \"tamil\"]\n",
    "    exclude = [\"<s>\", \"</s>\", \"START\", \"END\"]\n",
    "    wordtypes = []\n",
    "    tagscount = []\n",
    "\n",
    "    # Open training file to read the contents\n",
    "    f = codecs.open(filepath[int(sys.argv[1])], \"r\", encoding=\"utf-8\")\n",
    "    file_contents = f.readlines()\n",
    "\n",
    "    # Initialize count of each tag to Zero's\n",
    "    for _x in range(len(tags)):\n",
    "        tagscount.append(0)\n",
    "\n",
    "    # Calculate count of each tag in the training corpus and also the wordtypes in the corpus\n",
    "    for _x in range(len(file_contents)):\n",
    "        line = file_contents.pop(0).strip().split(\" \")\n",
    "        for i, word in enumerate(line):\n",
    "            if i == 0:\n",
    "                if word not in wordtypes and word not in exclude:\n",
    "                    wordtypes.append(word)\n",
    "            else:\n",
    "                if word in tags and word not in exclude:\n",
    "                    tagscount[tags.index(word)] += 1\n",
    "    f.close()\n",
    "\n",
    "    # Declare variables for emission and transmission matrix\n",
    "    emission_matrix = []\n",
    "    transmission_matrix = []\n",
    "\n",
    "    # Initialize emission matrix\n",
    "    for x in range(len(tags)):\n",
    "        emission_matrix.append([])\n",
    "        for _y in range(len(wordtypes)):\n",
    "            emission_matrix[x].append(0)\n",
    "\n",
    "    # Initialize transmission matrix\n",
    "    for x in range(len(tags)):\n",
    "        transmission_matrix.append([])\n",
    "        for _y in range(len(tags)):\n",
    "            transmission_matrix[x].append(0)\n",
    "\n",
    "    # Open training file to update emission and transmission matrix\n",
    "    f = codecs.open(filepath[int(sys.argv[1])], \"r\", encoding=\"utf-8\")\n",
    "    file_contents = f.readlines()\n",
    "\n",
    "    # Update emission and transmission matrix with appropriate counts\n",
    "    row_id = -1\n",
    "    for _ in range(len(file_contents)):\n",
    "        line = file_contents.pop(0).strip().split(\" \")\n",
    "\n",
    "        if line[0] not in exclude:\n",
    "            col_id = wordtypes.index(line[0])\n",
    "            prev_row_id = row_id\n",
    "            row_id = tags.index(line[1])\n",
    "            emission_matrix[row_id][col_id] += 1\n",
    "            if prev_row_id != -1:\n",
    "                transmission_matrix[prev_row_id][row_id] += 1\n",
    "        else:\n",
    "            row_id = -1\n",
    "\n",
    "    # Divide each entry in emission matrix by appropriate tag count to store probabilities in each entry instead of just count\n",
    "    for x in range(len(tags)):\n",
    "        for y in range(len(wordtypes)):\n",
    "            if tagscount[x] != 0:\n",
    "                emission_matrix[x][y] = float(emission_matrix[x][y]) / tagscount[x]\n",
    "\n",
    "    # Divide each entry in transmission matrix by appropriate tag count to store probabilities in each entry instead of just count\n",
    "    for x in range(len(tags)):\n",
    "        for y in range(len(tags)):\n",
    "            if tagscount[x] != 0:\n",
    "                transmission_matrix[x][y] = float(transmission_matrix[x][y]) / tagscount[x]\n",
    "\n",
    "    print(time.time() - start_time, \"seconds for training\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # End of Training Phase\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Start of Testing Phase\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Open the testing file to read test sentences\n",
    "    testpath = sys.argv[2]\n",
    "    file_test = codecs.open(testpath, \"r\", encoding=\"utf-8\")\n",
    "    test_input = file_test.readlines()\n",
    "\n",
    "    # Declare variables for test words and pos tags\n",
    "    test_words = []\n",
    "    pos_tags = []\n",
    "\n",
    "    # Create an output file to write the output tags for each sentences\n",
    "    file_output = codecs.open(\"./output/\" + languages[int(sys.argv[1])] + \"_tags.txt\", \"w\", \"utf-8\")\n",
    "    file_output.close()\n",
    "\n",
    "    # For each line POS tags are computed\n",
    "    for _j in range(len(test_input)):\n",
    "\n",
    "        test_words = []\n",
    "        pos_tags = []\n",
    "\n",
    "        line = test_input.pop(0).strip().split(\" \")\n",
    "\n",
    "        for word in line:\n",
    "            test_words.append(word)\n",
    "            pos_tags.append(-1)\n",
    "\n",
    "        viterbi_matrix = []\n",
    "        viterbi_path = []\n",
    "\n",
    "        # Initialize viterbi matrix of size |tags| * |no of words in test sentence|\n",
    "        for x in range(len(tags)):\n",
    "            viterbi_matrix.append([])\n",
    "            viterbi_path.append([])\n",
    "            for _ in range(len(test_words)):\n",
    "                viterbi_matrix[x].append(0)\n",
    "                viterbi_path[x].append(0)\n",
    "\n",
    "        # Update viterbi matrix column wise\n",
    "        for x in range(len(test_words)):\n",
    "            for y in range(len(tags)):\n",
    "                if test_words[x] in wordtypes:\n",
    "                    word_index = wordtypes.index(test_words[x])\n",
    "                    tag_index = tags.index(tags[y])\n",
    "                    emission = emission_matrix[tag_index][word_index]\n",
    "                else:\n",
    "                    emission = 0.001\n",
    "\n",
    "                if x > 0:\n",
    "                    max, viterbi_path[y][x] = max_connect(x, y, viterbi_matrix, emission, transmission_matrix)\n",
    "                else:\n",
    "                    max = 1\n",
    "                viterbi_matrix[y][x] = emission * max\n",
    "\n",
    "        # Identify the max probability in last column i.e. best tag for last word in test sentence\n",
    "        maxval = -999999\n",
    "        maxs = -1\n",
    "        for x in range(len(tags)):\n",
    "            if viterbi_matrix[x][len(test_words) - 1] > maxval:\n",
    "                maxval = viterbi_matrix[x][len(test_words) - 1]\n",
    "                maxs = x\n",
    "\n",
    "        # Backtrack and identify best tags for each words\n",
    "        for x in range(len(test_words) - 1, -1, -1):\n",
    "            pos_tags[x] = maxs\n",
    "            maxs = viterbi_path[maxs][x]\n",
    "\n",
    "        # Display POS Tags in the console.\n",
    "        # print pos_tags\n",
    "\n",
    "        # Print output to the file.\n",
    "        file_output = codecs.open(\"./output/\" + languages[int(sys.argv[1])] + \"_tags.txt\", \"a\", \"utf-8\")\n",
    "        for i, x in enumerate(pos_tags):\n",
    "            file_output.write(test_words[i] + \"_\" + tags[x] + \" \")\n",
    "        file_output.write(\" ._.\\n\")\n",
    "\n",
    "    f.close()\n",
    "    file_output.close()\n",
    "    file_test.close()\n",
    "\n",
    "    print(time.time() - start_time, \"seconds for testing 100 Sentences\")\n",
    "\n",
    "    print(\"\\nKindly check ./output/\" + languages[int(sys.argv[1])] + \"_tags.txt file for POS tags.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Execution begins here\n",
    "# --------------------------------------------------------------------------\n",
    "# \tDescription\n",
    "#\n",
    "# \tIf all the libraries are available in target system, main() is called,\n",
    "# \telse program exits gracefully by displaying dependancies.\n",
    "#\n",
    "# --------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import codecs\n",
    "        import os\n",
    "        import sys\n",
    "        import time\n",
    "\n",
    "        if len(sys.argv) == 3:\n",
    "            main()\n",
    "        else:\n",
    "            print(\"Usage: python supervised.py <language> <test_file_path>\")\n",
    "            print(\"Example: python supervised.py 0 ./data/hindi_testing.txt\")\n",
    "            print(\"More Info: Check ./Readme - Supervised.txt for detailed information\")\n",
    "\n",
    "    except ImportError as error:\n",
    "        print(f\"Couldn't find the module - {error.message[16:]}, kindly install before proceeding.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
